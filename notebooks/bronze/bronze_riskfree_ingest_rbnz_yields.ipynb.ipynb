{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e5f9adb-8841-4383-ad42-c45579935b33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data ETL Notebook\n",
    "\n",
    "**Layer**: Bronze\n",
    "\n",
    "**Domain**: Risk-free\n",
    "\n",
    "**Action**: Ingest RBNZ Yields and Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8387afd-2aaa-4891-89c6-0fbedd2220c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The purpose of this notebook is to ingest the file hb2-daily-close.xlsx from raw_data volume, apply SCD Type 2 data lineage, and write to a bronze data table with full history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba49020-450f-46a0-9280-1b58f73902ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install project requirements\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "171cb1cc-0cd0-46e7-a0f4-9519f57d9bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "from delta.tables import DeltaTable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e66b152-ca8b-4197-8813-3f38c6976166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define path for input data file\n",
    "source_directory = '/Volumes/workspace/riskfree_bronze/raw_data/'\n",
    "source_file_names = [f for f in os.listdir(source_directory) if os.path.isfile(os.path.join(source_directory, f))]\n",
    "source_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02034602-ac4b-4529-8892-43e1a3faad9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check source file names\n",
    "valid_files = ['hb2-daily-close.xlsx', 'hb2-daily.xlsx']\n",
    "if not any(file in source_file_names for file in valid_files):\n",
    "    raise ValueError(\"Source file names must be one of: 'hb2-daily-close.xlsx', 'hb2-daily.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37009506-fb51-4759-b5cd-33f82cb449d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingestion_timestamp = datetime.datetime.now()\n",
    "print('Ingestion timestamp:', ingestion_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498ae8c9-28c6-4f78-8ae9-96bee40dc8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ingest Series Data into bronze table (overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dafe11c-90a8-4ed3-9731-32b0e285eb0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in source_file_names:\n",
    "\n",
    "    # Set location\n",
    "    excel_path = f\"{source_directory}{file}\"\n",
    "\n",
    "    # Load 'Series Definitions' sheet\n",
    "    df_series = pd.read_excel(excel_path, sheet_name=\"Series Definitions\")\n",
    "\n",
    "    # Clean column names\n",
    "    df_series.columns = [col.strip().replace(\" \", \"_\").lower() for col in df_series.columns]\n",
    "\n",
    "    # Add source info\n",
    "    df_series[\"source_file_name\"] = file\n",
    "    df_series[\"ingestion_timestamp\"] = ingestion_timestamp\n",
    "\n",
    "    # Convert to Spark and write to Delta\n",
    "    spark_series = spark.createDataFrame(df_series)\n",
    "\n",
    "    # Create the Delta table if it does not exist\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS workspace.riskfree_metadata.series_definitions (\n",
    "        series_id STRING,\n",
    "        group STRING,\n",
    "        series STRING,\n",
    "        unit STRING,\n",
    "        note STRING,\n",
    "        source_file_name STRING,\n",
    "        ingestion_timestamp TIMESTAMP,\n",
    "        PRIMARY KEY (series_id)\n",
    "    ) USING DELTA\n",
    "    \"\"\")\n",
    "\n",
    "    delta_series_table = DeltaTable.forName(spark, \"workspace.riskfree_metadata.series_definitions\")\n",
    "    delta_series_table.alias(\"t\").merge(\n",
    "        spark_series.alias(\"s\"),\n",
    "        \"t.series_id = s.series_id\"\n",
    "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "    print(\"Series metadata saved to workspace.riskfree_metadata.series_definitions from \", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33ca1146-3147-4f60-b889-e13c1eaab33d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ingest Nominal Yield Data into bronze table (append, SCD Type 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee697da2-19cf-493d-8c1f-930e1823cc73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in source_file_names:\n",
    "\n",
    "    # Set location\n",
    "    excel_path = f\"{source_directory}{file}\"\n",
    "\n",
    "    # Get publish date\n",
    "    df = pd.read_excel(excel_path, sheet_name=\"Table Description\")\n",
    "    publish_date = pd.to_datetime(df[df[\"Published By\"] == \"Published Date\"][\"Reserve Bank of New Zealand\"]).dt.strftime('%Y-%m-%d').values[0]\n",
    "\n",
    "    # Imports & Config\n",
    "    sheet_name = \"Data\"\n",
    "    bronze_table = \"workspace.riskfree_bronze.rbnz_yields_raw\"\n",
    "\n",
    "    # Load Excel Sheet\n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet_name, skiprows=4)\n",
    "    df = df.rename(columns={\"Series Id\": \"date\"})\n",
    "\n",
    "    df[\"source_file_name\"] = file\n",
    "    df[\"ingestion_timestamp\"] = ingestion_timestamp\n",
    "    df[\"publish_date\"] = publish_date\n",
    "\n",
    "    # Normalize to long format\n",
    "    df_long = pd.melt(\n",
    "        df,\n",
    "        id_vars=[\"date\", \"source_file_name\", \"ingestion_timestamp\", \"publish_date\"],\n",
    "        var_name=\"series_id\",\n",
    "        value_name=\"yield_percent\"\n",
    "    )\n",
    "    df_long = df_long.dropna(subset=[\"yield_percent\"])\n",
    "\n",
    "    # Convert to Spark & Add SCD2 fields\n",
    "    spark_df = spark.createDataFrame(df_long)\n",
    "    spark_df = spark_df \\\n",
    "        .withColumn(\"effective_start\", lit(ingestion_timestamp)) \\\n",
    "        .withColumn(\"effective_end\", lit(None).cast(\"timestamp\")) \\\n",
    "        .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "    # Create Bronze Table (if needed)\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {bronze_table} (\n",
    "        date DATE,\n",
    "        series_id STRING,\n",
    "        yield_percent DOUBLE,\n",
    "        publish_date DATE,\n",
    "        source_file_name STRING,\n",
    "        ingestion_timestamp TIMESTAMP,\n",
    "        effective_start TIMESTAMP,\n",
    "        effective_end TIMESTAMP,\n",
    "        is_current BOOLEAN\n",
    "    )\n",
    "    USING DELTA\n",
    "    \"\"\")\n",
    "\n",
    "    # SCD Type 2 Merge\n",
    "    delta_table = DeltaTable.forName(spark, bronze_table)\n",
    "    delta_table.alias(\"t\").merge(\n",
    "        spark_df.alias(\"s\"),\n",
    "        \"t.date = s.date AND t.series_id = s.series_id AND t.publish_date = s.publish_date AND t.is_current = true\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=\"t.yield_percent != s.yield_percent\",\n",
    "        set={\n",
    "            \"effective_end\": \"s.effective_start\",\n",
    "            \"is_current\": \"false\"\n",
    "        }\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "    print(\"✅ SCD Type 2 merge completed into:\", bronze_table, \" from \", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b96f397c-411d-43c9-a7be-639c01520630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Move the xlsx into archive folder with date name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef569526-b4b3-4e28-ad1b-b094176b0903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in source_file_names:\n",
    "\n",
    "    # Set location\n",
    "    excel_path = f\"{source_directory}{file}\"\n",
    "\n",
    "    # Imports & Config\n",
    "    sheet_name = \"Table Description\"\n",
    "\n",
    "    # Load Excel Sheet\n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "    publish_date = pd.to_datetime(df[df[\"Published By\"] == \"Published Date\"][\"Reserve Bank of New Zealand\"]).dt.strftime('%Y-%m-%d').astype(str).values[0]\n",
    "\n",
    "    # Excel archive name\n",
    "    excel_path_archive = f\"/Volumes/workspace/riskfree_bronze/raw_data/archive/{publish_date}-{file}\"\n",
    "\n",
    "    # Move the processed Excel file to the archive directory\n",
    "    dbutils.fs.mv(excel_path, excel_path_archive)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8440540639228258,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_riskfree_ingest_rbnz_yields.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
