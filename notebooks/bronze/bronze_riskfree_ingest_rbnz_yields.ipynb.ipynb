{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e5f9adb-8841-4383-ad42-c45579935b33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data ETL Notebook\n",
    "\n",
    "**Layer**: Bronze\n",
    "\n",
    "**Domain**: Risk-free\n",
    "\n",
    "**Action**: Ingest RBNZ Yields and Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8387afd-2aaa-4891-89c6-0fbedd2220c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The purpose of this notebook is to ingest the file hb2-daily-close.xlsx from raw_data volume, apply SCD Type 2 data lineage, and write to a bronze data table with full history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba49020-450f-46a0-9280-1b58f73902ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install project requirements\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "171cb1cc-0cd0-46e7-a0f4-9519f57d9bf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37009506-fb51-4759-b5cd-33f82cb449d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define path for input data file\n",
    "source_file_name = 'hb2-daily-close.xlsx'\n",
    "excel_path = f\"/Volumes/workspace/riskfree_bronze/raw_data/{source_file_name}\"\n",
    "ingestion_timestamp = datetime.datetime.now()\n",
    "print('Ingestion timestamp:', ingestion_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498ae8c9-28c6-4f78-8ae9-96bee40dc8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ingest Series Data into bronze table (overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dafe11c-90a8-4ed3-9731-32b0e285eb0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load 'Series Definitions' sheet\n",
    "df_series = pd.read_excel(excel_path, sheet_name=\"Series Definitions\")\n",
    "\n",
    "# Clean column names\n",
    "df_series.columns = [col.strip().replace(\" \", \"_\").lower() for col in df_series.columns]\n",
    "\n",
    "# Add source info\n",
    "df_series[\"source_file_name\"] = source_file_name\n",
    "df_series[\"ingestion_timestamp\"] = ingestion_timestamp\n",
    "\n",
    "# Convert to Spark and write to Delta\n",
    "spark_series = spark.createDataFrame(df_series)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS workspace.riskfree_metadata\n",
    "\"\"\")\n",
    "\n",
    "spark_series.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.riskfree_metadata.series_definitions\")\n",
    "\n",
    "print(\"Series metadata saved to workspace.riskfree_metadata.series_definitions\")\n",
    "display(df_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33ca1146-3147-4f60-b889-e13c1eaab33d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ingest Nominal Yield Data into bronze table (append, SCD Type 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee697da2-19cf-493d-8c1f-930e1823cc73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports & Config\n",
    "sheet_name = \"Data\"\n",
    "bronze_table = \"workspace.riskfree_bronze.rbnz_yields_raw\"\n",
    "\n",
    "# Load Excel Sheet\n",
    "df = pd.read_excel(excel_path, sheet_name=sheet_name, skiprows=4)\n",
    "df = df.rename(columns={\"Series Id\": \"date\"})\n",
    "\n",
    "df[\"source_file_name\"] = source_file_name\n",
    "df[\"ingestion_timestamp\"] = ingestion_timestamp\n",
    "\n",
    "# Normalize to long format\n",
    "df_long = pd.melt(\n",
    "    df,\n",
    "    id_vars=[\"date\", \"source_file_name\", \"ingestion_timestamp\"],\n",
    "    var_name=\"series_id\",\n",
    "    value_name=\"yield_percent\"\n",
    ")\n",
    "df_long = df_long.dropna(subset=[\"yield_percent\"])\n",
    "\n",
    "# Convert to Spark & Add SCD2 fields\n",
    "spark_df = spark.createDataFrame(df_long)\n",
    "spark_df = spark_df \\\n",
    "    .withColumn(\"effective_start\", lit(ingestion_timestamp)) \\\n",
    "    .withColumn(\"effective_end\", lit(None).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "# Create Bronze Table (if needed)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {bronze_table} (\n",
    "    date DATE,\n",
    "    series_id STRING,\n",
    "    yield_percent DOUBLE,\n",
    "    source_file_name STRING,\n",
    "    ingestion_timestamp TIMESTAMP,\n",
    "    effective_start TIMESTAMP,\n",
    "    effective_end TIMESTAMP,\n",
    "    is_current BOOLEAN\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# SCD Type 2 Merge\n",
    "delta_table = DeltaTable.forName(spark, bronze_table)\n",
    "delta_table.alias(\"t\").merge(\n",
    "    spark_df.alias(\"s\"),\n",
    "    \"t.date = s.date AND t.series_id = s.series_id AND t.is_current = true\"\n",
    ").whenMatchedUpdate(\n",
    "    condition=\"t.yield_percent != s.yield_percent\",\n",
    "    set={\n",
    "        \"effective_end\": \"s.effective_start\",\n",
    "        \"is_current\": \"false\"\n",
    "    }\n",
    ").whenNotMatchedInsertAll().execute()\n",
    "\n",
    "print(\"âœ… SCD Type 2 merge completed into:\", bronze_table)\n",
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b96f397c-411d-43c9-a7be-639c01520630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Move the xlsx into archive folder with date name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef569526-b4b3-4e28-ad1b-b094176b0903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports & Config\n",
    "sheet_name = \"Table Description\"\n",
    "# Load Excel Sheet\n",
    "df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "publish_date = pd.to_datetime(df[df[\"Published By\"] == \"Published Date\"][\"Reserve Bank of New Zealand\"]).dt.strftime('%Y-%m-%d').astype(str).values[0]\n",
    "\n",
    "# Excel archive name\n",
    "excel_path_archive = f\"/Volumes/workspace/riskfree_bronze/raw_data/archive/{publish_date}-{source_file_name}\"\n",
    "\n",
    "# Move the processed Excel file to the archive directory\n",
    "dbutils.fs.mv(excel_path, excel_path_archive)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8440540639228258,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_riskfree_ingest_rbnz_yields.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
